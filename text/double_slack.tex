\section{Update rules}

We are interested in solving a passive aggressive style problem only that we are shown $k_t$ examples at time $t$. Specifically, we are interested in the case that the samples arrive from different classes. We show that by using a balanced regiem we can provide bounds for the AUC and for a multiclass AUC. The specific method we choose to optimize the problem have a great deal of implication on the solution where different steps will yield different results.

\begin{equation*}
\begin{aligned}
& \underset{\vw}{\text{minimize}}
& & \frac{1}{2} || \vw - \vw_t ||^2 + C \sum\limits_{k=1}^{k_t}{\xi^k} \\
& \text{subject to}
& & 1 - \vw^T \vx^k \vy^k \leq \xi^k, \;
 \; k = 1, \ldots, k_t.
\end{aligned}
\end{equation*}


The dual problem is 
\begin{equation*}
\begin{aligned}
& \underset{\valpha}{\text{maximize}}
& & \frac{1}{2} || \sum\limits_{i=1}^{k_t} {\alpha_i \vx^k y^k}||^2 + \sum\limits_{k=1}^{k_t} {\alpha_i (1 - \vw_t^T \vx^k \vy^k }) \\
& \text{subject to}
& & 0 \leq \alpha_i \leq C, \;
 \; k = 1, \ldots, k_t.
\end{aligned}
\end{equation*}


This problem does not have a closed analytic solution, So we aim to maximize the dual function iterativly. One way of solving this is using \textit{dual coordinate ascent} (DCA) on all the $k_t$ samples. DCA will make iteration until convergence. However as noticed by ...,  we can also make a small advancement that is not optimal but that will advance us in the right direction. For example the passive aggressive algorithm can be thought of as single iteration of DCA where we update each dual variable only once. We suggest that this can be extended by advancing with a step that is made using the joined information of several samples. We choose a set of indices $\mathcal{J} $ to increase using the \textbf{ same } step.

\[ \alpha_j = \alpha_j + \tau , \;\; j \in \mathcal{J} \]
We derive the following $\tau$
\[ \tau = max(L_b   , min(U_b , \frac{|\mathcal{J}| - \vw_t^T \sum\limits_{j \in \mathcal{J}} {\vx^j y^j}   }{|| \sum\limits_{j \in \mathcal{J} }{\vx^j y^j} ||^2 }  ) )
\]
Where $ L_b = max_{ j \in \mathcal{J} } (-\alpha_j) $ and $ U_b = min_{ j \in \mathcal{J} } (C -\alpha_j) $ which appear since each of the updated $\alpha_j$ needs to keep its constrain $0 \leq \alpha_j \leq C $.
It is possible that a step that advances all $j \in \mathcal{J} $ does not exists because $ U_b $ could be smaller than $L_b$. We will always be able to perform at least one such step since we initialize $\alpha_i$ as zero. If we partition the set of samples and at each iteration we use a different partition, we are guaranteed that $L_b \leq U_b $ since we advance all the dual variables in each partition with the same steps.\\



We can think of this as updating a new vector $\vx^\mathcal{J} = \sum\limits_{j \in \mathcal{J} }{\vx^j y^j}$ where $y^\mathcal{J} = 1 $. Only here $\vx^\mathcal{J}$ should be correct with a margin of $|\mathcal{J}| $.  We than update $\vw$ using $\vx^\mathcal{J}$ with the step size $\tau$. By using multiple items in $\mathcal{J}$ we make a statement about their linear combination and not any of them individually. For example, when we update two items a positive sample $\vxp$ and a negative sample $\vxn$ forcing that their sum should be classified positive $0 \leq \vw^T (\vxp - \vxn) $ we actually argue about their order we say that their difference should be kept positive or that $ \vw^T \vxn \leq \vw^T \vxp$.\\


Using various sets of $ \mathcal{J} $ and various number of iteration at time $t$ we propose several update rules:\\

In the case where at time $t$ we are provided with two samples ($k_t=2$), $\vxp, \vxn$.\\
I. PA-DCA - Iterate until convergence at each iteration choose a single sample ($|\mathcal{J}|=1$).\\
II. PA-sequential - Iterate only once for $\vxp$ and than once for $\vxn$.\\
III. PA-AUC - Iterate only once using both samples $\mathcal{J} = \{\vxp, \vxn\} $ .\\
IV. PA-correctMistakes - Iterate only once use only the samples that failed to achieve correct classification with the margin. $\mathcal{J} = \{\vxp, \vxn\} \; or \;\{\vxp \} \; or \; \{ \vxn \}$.\\

In the case where we are presented $k$ samples. \\
I. PA-DCA - Iterate until convergence at each iteration choose a single sample ($|\mathcal{J}|=1$).\\
II. PA-sequential - Iterate only once for each sample.\\
III. PA-maxViolators - Iterate only once. Here $\mathcal{J}$ contains the positive sample that caused the highest loss and the negative sample that caused the highest loss. \\
IV. PA-correctMistakes - Iterate only once use only the samples that failed to achieve correct classification with the margin. $\mathcal{J} = \{ i | 0 \leq l_{w_t}(\vx_i, y_i) \}$.\\


Theorem 4: convergence of DCA

Theorem 5: classification errors --> number of mistakes

Theorem 6: As in the case of the classical passive aggressive if our step is not caped by C after the update we will correctly classify the samples. In case where we are caped by C the loss of the samples we choose to update will decrease. But we are not guarantied a correct classification. \\
Show that classification is correct and $ w x+ >0$ while $w x- < 0$ after the update when it is not caped.

Theorem 7: from Theorem 5 it follows that 1-AUC is bounded
