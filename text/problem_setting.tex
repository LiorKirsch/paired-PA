\section{Problem Setting}

In this section we introduce the notation used throughout the paper and describe our problem setting. Vectors are denoted by lower case bold face letters (e.g. $\vx$ and $\vw$) 
%where the $i^{th}$ element of the vector $\vx$ is denoted by $\vx_i$. 
The hinge function is denoted by $[x]_+ = max\{0, x\} $. Sets of indices are denoted by capital curly letters (e.g. $\mathcal{J}$). We denote samples which arrive from the $k$ class using superscript (e.g. $\vx^k$). Subscript will denote the time point the samples is introduced (e.g. $\vx^k_t$ is a vector from the $k$ class at time $t$). \\

We are interested in the case where at each time point $t$ we receive a batch of $k_t$ sample and than choose how to update the vector weights $\vw$. Specifically, we are interest in the case where each of the $k_t$ samples arrives from a different class. At each time point $t$ we solve an optimization problem which performs a trade off between two things. First, it aims that the new solution $\vw$ will be close to the former weight vector $w_t$. Second we prefer to classify all the samples provided at time $t$ correctly with a margin of 1.

\begin{equation*}
\begin{aligned}
& \underset{\vw}{\text{minimize}}
& & \frac{1}{2} || \vw - \vw_t ||^2 + C \sum\limits_{k=1}^{k_t}{\xi^k} \\
& \text{subject to}
& & 1 - \vw^T \vx^k y^k \leq \xi^k, \;
 \; k = 1, \ldots, k_t.
\end{aligned}
\end{equation*}

There are some benefits of an update which uses several samples for the update. First, in cases where the data is unbalanced, using a balanced updating scheme that introduce an equal number of samples at time point $t$ we can come up with guaranties both for the classification mistakes and for the AUC.
Second, an update rule that uses several samples at a single time point $t$ is internally tuned since the we need to advanced $\vw$ in a way that is agreeable with the samples at time $t$. In a way the other classes are controlling the step size that is made. Much like the case in multibatch stochastic gradient descent, one of the benefits is that the steps are more moderate and there is less variability in each small step. Only here we are using the variability between different classes and not the variability within the class.