\section{Average Classification Loss}

We first start by examining the case where we have only two classes. We denote our classes with $X^+$ and $X^-$.

\begin{equation*}
\begin{aligned}
& \underset{\vw}{\text{minimize}}
& & \frac{1}{2} || \vw - \vw_t ||^2 + C ( \xi^+ + \xi^- ) \\
& \text{subject to}
& & 1 - \vw^T \vx^+ \leq \xi^+\\
& & & 1 + \vw^T \vx^-  \leq \xi^- \\
& & & 0 \leq \xi^+, \xi^-
\end{aligned}
\end{equation*}

The dual problem is 
\begin{equation*}
\begin{aligned}
& \underset{\alpha^+, \alpha^-}{\text{maximize}}
& & \frac{1}{2} || \alpha^+ \vx^+ - \alpha^- \vx^-||^2 + \alpha^+ (1 - \vw_t^T \vx^+ ) + \alpha^- (1 + \vw_t^T \vx^- )\\
& \text{subject to}
& & 0 \leq \alpha^+,\alpha^- \leq C, 
\end{aligned}
\end{equation*}
 and the relation between the dual variables and the primial variables is: $\vw = \vw_t + \alpha^+ \vx^+ - \alpha^- \vx^- $.

First notice that by adding the two constrains we get $1 - \vw^T \frac{(\vx^+ - \vx^- )}{2} \leq \frac{\xi^+ + \xi^-}{2}$. By replacing the two constrain with this new constrain we get a type of problem that we refer to as $PA_{AUC}$ \cite{Keshet2009}. By solving this problem we aim at insuring that the $\vxp$ and $\vxn$ are ranked correctly ($ \vw^T \vxn \leq \vw^T \vxp $).  By solving the problem with the two constrains, we are not only guarantied that we $\vxp$ will be classified higher than $\vxn$ but also that we classify the two samples correctly. 

AUC is often used in the case where we aim at classifying between two imbalanced classes. In such scenarios, minimizing the number of overall error is usually not helpful. For example, in the case where we have 1 positive for every 99 negative examples, a classifier which always predict negative will make 1\% errors. Usually, this type of classifier in not what we are interested in. AUC can be interpreted as the probability that a positive samples will be ranked higher than a negative sample. It is easy to see how this definition of AUC as a probability ranker helps in the imbalanced case:   By maximizing the AUC, we always treat a pair of samples, one positive and one negative in a balanced manner, regardless of their distribution in the overall population. This insures that the classes are treated as equals and that they are both equally represented to our classifier. However, maximizing the AUC insures only the order of the two samples; we are not guarantied a correct classification. And, is it so often happen that algorithms which maximize the AUC fail to provide correct classification and a dynamic threshold term is introduced. Many times, when we are in the scenario where we want to classify a pair of imbalanced classes, what we actually want is to minimize the the mean number of classification errors in the first class and to minimize number of classification errors in the second class. First we will show that the AUC-error (1 - AUC) can be bounded by the mean number of mistakes made in the first class and the mean number of mistakes made in the second class. This will later help us develop an update that is both good at classification and can be used in the scenario of imbalanced classes.

{\bf Theorem 1} {\it 1 - AUC is bounded by the mean 
of the errors in the first class and the mean number of errors in the second
class.  $1 - AUC \leq E(M^+) + E(M^-)$ 
}

{\bf Proof} 
\begin{multline}
1 - AUC = \frac{1}{|X^-| |X^+|} \sum\limits_{ \substack{x^+ \in X^+ \\ x^- \in X^-}} {  \mathbbm{1}_{\vw^T \vx^+ \leq \vw^T \vx^-} } \;\;\leq \\
\frac{1}{|X^-| |X^+|}  \sum\limits_{ \substack{x^+ \in X^+ \\ x^- \in X^-}} {  \mathbbm{1}_{\vw^T \vxp \leq 0} \;+\; \mathbbm{1}_{0 \leq \vw^T \vxn}  }  \;\;= \\
\frac{1}{|X^+|}  \sum\limits_{\vxp \in X^+}{  \mathbbm{1}_{\vw^T \vxp \leq 0} } \;+\; \frac{1}{|X^-|}  \sum\limits_{\vxn \in X^-}{\mathbbm{1}_{0 \leq \vw^T \vxn }  }   \;= \;
E(M^+) + E(M^-)
\end{multline}
\hfill\BlackBox \\


We will next show that we can provide bounds for the sum of the mean number of errors using several update rules.


Theorem 3: Show that classification is correct and $\vxp >0$ while $\vxn < 0$ after the update - this is not correct since we are balancing with $w_t$ from the former step.
This would have been correct if we would have posed this is a feasibility problem (perceptron style).
